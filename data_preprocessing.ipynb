{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview and Preprocessing\n",
    "\n",
    "Before proceeding to the stage of data analysis and building a supply and demand model, it is necessary to do data processing.\n",
    "\n",
    "Data Preprocessing includes the following actions:\n",
    "1. Checking the absence of rows with metrics in datasets\n",
    "2. Filling in the blanks\n",
    "3. Replacement null values with zero (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tabulate\n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.3\n",
      "IPython 7.2.0\n",
      "\n",
      "pandas 1.1.5\n",
      "tabulate 0.8.9\n",
      "datetime unknown\n",
      "typing 3.6.6\n",
      "\n",
      "compiler   : GCC 4.9.2\n",
      "system     : Linux\n",
      "release    : 4.19.0-12-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 56\n",
      "interpreter: 64bit\n",
      "Git hash   :\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p pandas,tabulate,datetime,typing -g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for processing the data and basic transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_overview(csv_path: str, date_columns: List[str]) -> pd.DataFrame():\n",
    "    '''\n",
    "    Function returns dataset from csv file as pd.DataFrame() and prints dataset overview \n",
    "    \n",
    "    input:\n",
    "    csv_path: str – path csv file with dataset\n",
    "    date_columns: List[str] – date columns to parse into datetime object\n",
    "    \n",
    "    output: pd.DataFrame()\n",
    "    '''\n",
    "    df = pd.read_csv(csv_path, parse_dates=date_columns, infer_datetime_format=True)\n",
    "    print(f'{csv_path}\\nDataset shape: {df.shape}')\n",
    "    print('\\nFirst 10 rows:')\n",
    "    print(tabulate.tabulate(df[:10].values, df.columns, tablefmt=\"pipe\"))\n",
    "    print(f'\\nCheck for Null values:\\n{df.isna().sum()}')\n",
    "    return df\n",
    "\n",
    "def find_missing_dates(df: pd.DataFrame(), date_column: str) -> List[datetime.datetime]:\n",
    "    '''\n",
    "    Function returns list of missing dates in specified dataframe\n",
    "    \n",
    "    input:\n",
    "    df: pd.DataFrame() – dataframe to search for missing dates\n",
    "    date_column: str – date column \n",
    "    \n",
    "    output: List[datetime.datetime]\n",
    "    '''\n",
    "    df = df[date_column].groupby([df[date_column].dt.date]).count().to_frame()\n",
    "    df.columns = ['Rows_Count']\n",
    "    df = df.reset_index()\n",
    "    missing_dates = list(df[date_column][df['Rows_Count'] < 24])\n",
    "    print(missing_dates)\n",
    "    return missing_dates\n",
    "    \n",
    "def create_missing_rows(df: pd.DataFrame(), \n",
    "                        date_column: str, \n",
    "                        missing_dates: List[datetime.datetime]) -> pd.DataFrame():\n",
    "    '''\n",
    "    Function creates dataframe of missing rows in specified dataframe\n",
    "    \n",
    "    input:\n",
    "    df: pd.DataFrame() – dataframe with missing rows\n",
    "    date_column: str – date column \n",
    "    missing_dates: List[datetime.datetime] – missing dates to fill the rows \n",
    "    \n",
    "    output: updated dataframe pd.DataFrame()\n",
    "    '''\n",
    "    df_missing_rows = pd.DataFrame(columns= df.columns)\n",
    "    hour_list = range(24)\n",
    "    for date in missing_dates:\n",
    "        df_hours = list(df[date_column][df[date_column].dt.date == date].dt.hour)\n",
    "        for hour in hour_list:\n",
    "            if hour not in df_hours:\n",
    "                date = datetime.datetime.combine(date, datetime.time(hour=hour))\n",
    "                df_missing_rows=df_missing_rows.append({date_column: date}, ignore_index=True)\n",
    "                df_missing_rows.fillna(value=0, inplace=True)    \n",
    "    print(tabulate.tabulate(df_missing_rows.values, df_missing_rows.columns, tablefmt=\"pipe\"))\n",
    "    return df_missing_rows\n",
    "    \n",
    "def insert_missing_row(df: pd.DataFrame(), df_missing_rows: pd.DataFrame()) -> pd.DataFrame():\n",
    "    '''\n",
    "    Function inserts missimg rows to specified dataframe\n",
    "    \n",
    "    input:\n",
    "    df: pd.DataFrame() – dataframe with missing rows\n",
    "    df_missing_rows: pd.DataFrame() – dataframe of missing rows\n",
    "    \n",
    "    output: updated dataframe pd.DataFrame()\n",
    "    '''\n",
    "    df = pd.concat([df, df_missing_rows], ignore_index=True)\n",
    "    print(f'New dataset shape: {df.shape}')\n",
    "    print(f'\\nInserted row:')\n",
    "    print(tabulate.tabulate(df[-len(df_missing_rows):].values, df.columns, tablefmt=\"pipe\"))\n",
    "    return df\n",
    "\n",
    "def replace_null_with_zero(df: pd.DataFrame(), date_columns: List[str]) -> pd.DataFrame():\n",
    "    '''\n",
    "    Function replaces Null values with zero (0)\n",
    "    \n",
    "    input:\n",
    "    df: pd.DataFrame() – dataframe with Null values\n",
    "    date_columns: List[str] – date columns to exclude\n",
    "    \n",
    "    output: updated dataframe pd.DataFrame()\n",
    "    '''\n",
    "    for column in df.columns:\n",
    "        if column not in date_columns:\n",
    "            if df[column].isna().sum() > 0:\n",
    "                df[column].fillna(value=0, inplace=True)\n",
    "    print(f'Final check for Nulls:\\n{df.isna().sum() > 0}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supply Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Hourly_DriverActivity_1.csv\n",
      "Dataset shape: (840, 9)\n",
      "\n",
      "First 10 rows:\n",
      "| Date                |   Active drivers |   Online (h) |   Has booking (h) |   Waiting for booking (h) |   Busy (h) |   Hours per active driver |   Rides per online hour |   Finished Rides |\n",
      "|:--------------------|-----------------:|-------------:|------------------:|--------------------------:|-----------:|--------------------------:|------------------------:|-----------------:|\n",
      "| 2016-12-18 23:00:00 |               52 |           18 |                 6 |                        11 |         11 |                       0.3 |                    0.67 |               12 |\n",
      "| 2016-12-18 22:00:00 |               59 |           20 |                11 |                         9 |         12 |                       0.3 |                    1.4  |               28 |\n",
      "| 2016-12-18 21:00:00 |               72 |           25 |                 7 |                        18 |         15 |                       0.3 |                    0.64 |               16 |\n",
      "| 2016-12-18 20:00:00 |               86 |           29 |                 7 |                        23 |         15 |                       0.3 |                    0.52 |               15 |\n",
      "| 2016-12-18 19:00:00 |               82 |           31 |                14 |                        17 |         19 |                       0.4 |                    1.16 |               36 |\n",
      "| 2016-12-18 18:00:00 |               81 |           32 |                14 |                        18 |         18 |                       0.4 |                    1    |               32 |\n",
      "| 2016-12-18 17:00:00 |               72 |           30 |                13 |                        17 |         13 |                       0.4 |                    0.87 |               26 |\n",
      "| 2016-12-18 16:00:00 |               68 |           23 |                 9 |                        13 |         13 |                       0.3 |                    1.04 |               24 |\n",
      "| 2016-12-18 15:00:00 |               65 |           24 |                 5 |                        19 |         12 |                       0.4 |                    0.5  |               12 |\n",
      "| 2016-12-18 14:00:00 |               50 |           17 |                 7 |                        10 |         12 |                       0.3 |                    0.82 |               14 |\n",
      "\n",
      "Check for Null values:\n",
      "Date                        0\n",
      "Active drivers              0\n",
      "Online (h)                  0\n",
      "Has booking (h)             0\n",
      "Waiting for booking (h)     0\n",
      "Busy (h)                    0\n",
      "Hours per active driver     0\n",
      "Rides per online hour       0\n",
      "Finished Rides             45\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "supply_data = df_overview(csv_path='data/Hourly_DriverActivity_1.csv', date_columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "missing_dates = find_missing_dates(df=supply_data, date_column='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final check for Nulls:\n",
      "Date                       False\n",
      "Active drivers             False\n",
      "Online (h)                 False\n",
      "Has booking (h)            False\n",
      "Waiting for booking (h)    False\n",
      "Busy (h)                   False\n",
      "Hours per active driver    False\n",
      "Rides per online hour      False\n",
      "Finished Rides             False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "supply_data_new = replace_null_with_zero(df=supply_data, date_columns=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demand Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/Hourly_OverviewSearch_1.csv\n",
      "Dataset shape: (839, 4)\n",
      "\n",
      "First 10 rows:\n",
      "| Date                |   People saw 0 cars (unique) |   People saw +1 cars (unique) |   Coverage Ratio (unique) |\n",
      "|:--------------------|-----------------------------:|------------------------------:|--------------------------:|\n",
      "| 2016-12-18 23:00:00 |                            9 |                            32 |                        78 |\n",
      "| 2016-12-18 22:00:00 |                           29 |                            64 |                        69 |\n",
      "| 2016-12-18 21:00:00 |                            5 |                            39 |                        89 |\n",
      "| 2016-12-18 20:00:00 |                           13 |                            48 |                        79 |\n",
      "| 2016-12-18 19:00:00 |                           12 |                            77 |                        87 |\n",
      "| 2016-12-18 18:00:00 |                           13 |                            62 |                        83 |\n",
      "| 2016-12-18 17:00:00 |                            9 |                            61 |                        87 |\n",
      "| 2016-12-18 16:00:00 |                           20 |                            60 |                        75 |\n",
      "| 2016-12-18 15:00:00 |                            7 |                            37 |                        84 |\n",
      "| 2016-12-18 14:00:00 |                           16 |                            43 |                        73 |\n",
      "\n",
      "Check for Null values:\n",
      "Date                           0\n",
      "People saw 0 cars (unique)     0\n",
      "People saw +1 cars (unique)    0\n",
      "Coverage Ratio (unique)        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "demand_data = df_overview(csv_path='data/Hourly_OverviewSearch_1.csv', date_columns=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.date(2016, 12, 7)]\n"
     ]
    }
   ],
   "source": [
    "missing_dates = find_missing_dates(df=demand_data, date_column='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Date                |   People saw 0 cars (unique) |   People saw +1 cars (unique) |   Coverage Ratio (unique) |\n",
      "|:--------------------|-----------------------------:|------------------------------:|--------------------------:|\n",
      "| 2016-12-07 05:00:00 |                            0 |                             0 |                         0 |\n"
     ]
    }
   ],
   "source": [
    "df_missing_rows = create_missing_rows(df=demand_data, date_column='Date', missing_dates=missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset shape: (840, 4)\n",
      "\n",
      "Inserted row:\n",
      "| Date                |   People saw 0 cars (unique) |   People saw +1 cars (unique) |   Coverage Ratio (unique) |\n",
      "|:--------------------|-----------------------------:|------------------------------:|--------------------------:|\n",
      "| 2016-12-07 05:00:00 |                            0 |                             0 |                         0 |\n"
     ]
    }
   ],
   "source": [
    "demand_data_new = insert_missing_row(df=demand_data, df_missing_rows=df_missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_missing_dates(df=demand_data_new, date_column='Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save new datasets to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "supply_data_new.to_csv('data/Hourly_DriverActivity_1_Processed.csv', index=False)\n",
    "demand_data_new.to_csv('data/Hourly_OverviewSearch_1_Processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "At this stage of data processing, the following has been done:\n",
    "1. Replaced 45 empty  values  with zero (0 )in the Supply dataset in the column 'Finished Rides'\n",
    "2. Inserted a missing line in the Demand dataset for 2016-12-07 05:00:00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
